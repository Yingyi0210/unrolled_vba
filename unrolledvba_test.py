# -*- coding: utf-8 -*-
"""unrolledVBA_test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OrEt42kDchfGflbPsi5dVoI7EEV8BIg9

# Preprocess

## Data
Use a method works for you

### Mount Google Drive (Use VBA Google Account)
"""


################  Set file directory.  ################


########################################################


from tqdm import tqdm
from unroll_vba import *
import numpy as np
import torch
import os
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from torch import nn
import torch.optim as optim
from config import Args
from PIL import Image
from torchvision import transforms
from autoencoder import Autoencoder
import matplotlib.pyplot as plt
from skimage.metrics import peak_signal_noise_ratio, structural_similarity
import scipy.sparse
from dataset import SRDataset
from operators import *
import signal
from skimage.metrics import structural_similarity as ssim_metric

def compute_psnr(img1, img2):
    """计算两张 [0,1] 张量的 PSNR"""
    mse = F.mse_loss(img1, img2)
    psnr = 10 * torch.log10(1.0 / mse)
    return psnr.item()

hr_folder = r"D:\data\SR\DIV2K\DIV2K_train_HR_gray"
dataset = SRDataset(hr_folder, patch_size=256, scale=4, noise_std=0.01, augment=True)
#dataset = dataset[0]
#print(dataset['x'].shape, dataset['y'].shape, dataset['Aty'].shape)

# Test autoencoder model
#model = Autoencoder()

#input_image = torch.randn(1, 1, 64, 64) # Batch size 1, 1 channel, 256x256 image
#output_image = model(input_image)
#print("Input size: ", input_image.size())
#print("Output size: ", output_image.size())

"""### Trainer"""

def train_autoencoder(model, train_loader, num_epochs=10, learning_rate=0.001):
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    for epoch in range(num_epochs):
        for data in train_loader:
            y = data['y'].to(device)
            x = data['x'].to(device)
            # print("x size: ", x.size())
            # print("Input size: ", y.size())
            output = model(y)
            loss = criterion(output, x)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')


transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.ToTensor(),
])

# dataset = InfraredDataset(hr_dir, lr_dir, transform=transform)
# Test test_data with 2 samples
test_loader = DataLoader(dataset, batch_size=1, shuffle=True)
autoencoder = Autoencoder()
#train_autoencoder(autoencoder, test_loader)

"""## Unroll VBA model

"""### VBA Trainer


class Trainer(object):

    def __init__(self, args):
        self.model = args.model
        self.device = args.device
        self.batch_size = args.batch_size
        self.lr = args.lr
        self.dataset = args.dataset
        self.loss = args.loss
        self.optimizer = args.optimizer
        self.num_workers = args.num_workers
        self.epochs = args.epochs
        self.log_interval = args.log_interval
        self.img_save_dir = args.imgdir
        os.makedirs(self.img_save_dir, exist_ok=True)

    def _get_model(self):
        if self.model == 'autoencoder':
            model = Autoencoder()
            model.to(self.device)
        elif self.model == 'UnrollVBA':
            scale = 4
            model = UnrollVBA(block_num=4, scale=scale, width=256, height=256)
            model.to(self.device)
            # for name, param in model.named_parameters():
            #     if param.requires_grad:
            #         print(name)
        else:
            raise NotImplementedError("Unsupported model: {}".format(self.model))
        return model


    def _get_dataset(self):
        if self.dataset == 'infrared':
            # hr_folder = r"D:\data\SR\DIV2K\DIV2K_train_HR_gray"
            hr_folder = r"D:\data\SR\train\M3FD\Infrared"
            dataset = SRDataset(hr_folder, patch_size=256, scale=4, noise_std=0.01, augment=True,limit=60)
            # dataset = SRDataset(hr_folder, patch_size=256, scale=4, noise_std=0.01, augment=True)
        else:
            raise NotImplementedError("Unsupported dataset: {}".format(self.dataset))
        return dataset

    def _get_loss_func(self):
        if self.loss == 'mse':
            loss_fn = nn.MSELoss()
        elif self.loss == 'ce':
            loss_fn = nn.CrossEntropyLoss()
        elif self.loss == 'bce':
            loss_fn = nn.BCEWithLogitsLoss()
        else:
            raise NotImplementedError("Unsupported loss: {}".format(self.loss))
        return loss_fn

    def _get_optimizer(self, model):
        if self.optimizer == 'adam':
            optimizer = torch.optim.Adam(model.parameters(), lr=self.lr)
        elif self.optimizer == 'sgd':
            optimizer = torch.optim.SGD(model.parameters(), lr=self.lr, momentum=0.9)
        elif self.optimizer == 'adamW':
            optimizer = torch.optim.AdamW(model.parameters(), lr=self.lr)
        else:
            raise NotImplementedError("Unsupported optimizer: {}".format(self.optimizer))

        optimizer.param_groups[0]['lr'] = self.lr
        return optimizer

    def _save_reconstructed_image(self, recon, x_true,y, epoch, iter_num):
        """保存重构图像和真实图像"""
        # 确保图像在CPU上且为numpy格式，只取batch中的第一张图像
        recon_tensor = recon[7].detach().cpu()  # 获取张量
        recon = recon_tensor.squeeze().numpy()
        # recon_tensor_clamped = torch.clamp(recon_tensor, 0.0, 1.0)
        # recon = recon_tensor_clamped.squeeze().numpy()  # 转为numpy
        x_true = x_true[7].detach().cpu().squeeze().numpy()
        y = y[7].detach().cpu().squeeze().numpy()

        recon = (recon * 255).astype(np.uint8)
        x_true = (x_true * 255).astype(np.uint8)
        y = (y * 255).astype(np.uint8)

        # 创建图像保存路径
        save_path = os.path.join(self.img_save_dir, 'train2', f"epoch_{epoch}_iter_{iter_num}")
        os.makedirs(save_path, exist_ok=True)

        # 绘制并保存重构图像
        plt.figure(figsize=(12, 4))

        plt.subplot(1, 3, 1)
        plt.imshow(x_true, cmap='gray')
        plt.title('True Image')
        plt.axis('off')
        plt.savefig(os.path.join(save_path, 'true.png'))

        plt.subplot(1, 3, 2)
        plt.imshow(y, cmap='gray')
        plt.title('Observed (LR)')
        plt.axis('off')
        plt.savefig(os.path.join(save_path, 'LR.png'))

        plt.subplot(1, 3, 3)
        plt.imshow(recon, cmap='gray')
        plt.title('Reconstructed Image')
        plt.axis('off')
        plt.savefig(os.path.join(save_path, 'reconstructed.png'))

        plt.tight_layout()  # 调整子图间距
        plt.savefig(os.path.join(save_path, 'comparison.png'))

        # 绘制差异图
        # diff = np.abs(recon - x_true)
        # plt.subplot(1, 3, 3)
        # plt.imshow(diff, cmap='viridis')
        # plt.title('Difference')
        # plt.axis('off')
        # plt.colorbar()
        # plt.savefig(os.path.join(save_path, 'difference.png'))

        plt.close()

    def training(self, writer):
        dataset = self._get_dataset()
        print("Load dataset...")
        test_loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)
        loss_func = self._get_loss_func()
        model = self._get_model()

        optimizer = self._get_optimizer(model)
        # best_val_score = 0
        # early_stopper = EarlyStopping(patience=5)

        print("Start training...")
        losses, psnrs, ssims = [], [], []

        def plot_progress():
            """保存到当前epoch为止的曲线"""
            epochs = range(1, len(losses) + 1)
            plt.figure(figsize=(12, 4))
            plt.subplot(1, 3, 1)
            plt.plot(epochs, losses, marker='o')
            plt.title('Average Loss')
            plt.xlabel('Epoch')
            plt.grid(True)

            plt.subplot(1, 3, 2)
            plt.plot(epochs, psnrs, marker='o')
            plt.title('PSNR')
            plt.xlabel('Epoch')
            plt.grid(True)

            plt.subplot(1, 3, 3)
            plt.plot(epochs, ssims, marker='o')
            plt.title('SSIM')
            plt.xlabel('Epoch')
            plt.grid(True)

            plt.tight_layout()
            plt.savefig(os.path.join(self.img_save_dir, 'train2', f"progress_until_epoch_{len(losses)}.png"))
            plt.close()
            print(f"[INFO] 保存至 epoch {len(losses)} 的曲线 progress_until_epoch_{len(losses)}.png")

        def signal_handler(sig, frame):
            print("\n[INFO] 检测到中断信号，保存当前训练进度曲线...")
            plot_progress()  # 尝试保存图像
            exit(0)  # 正常退出

        signal.signal(signal.SIGINT, signal_handler)

        for epoch in range(self.epochs):
            model.train()
            train_loss = 0
            epoch_psnr, epoch_ssim = 0, 0
            count = 0
            # best_model_psnr = -1
            # best_model_path = os.path.join('results', "best_model.pth")
            best_psnr_in_epoch = -1
            best_recon = None
            best_x_true = None
            best_y = None

            # Training
            # for iter_num, data in enumerate(test_loader):
            loop = tqdm(enumerate(test_loader), total=len(test_loader), desc=f"Epoch {epoch + 1}/{self.epochs}")
            for iter_num, data in loop:
                # dataset returns a tuple (x, y, Aty)
                x_true, y, Aty = data
                x_true = x_true.to(self.device)
                y = y.to(self.device)
                Aty = Aty.to(self.device)
                mk_0 = F.interpolate(y, scale_factor=4, mode='bicubic', align_corners=False)
                mk = mk_0
                mk_1 = mk_0
                invSigmak_1, invSigmak, lambdak = self.initialization(Aty)
                diagAtA_np = np.load('diagAtA_bicubic2.npy')
                diagAtA = torch.from_numpy(diagAtA_np)
                diagAtA = torch.clamp(diagAtA, min=0.0)
                diagAtA = diagAtA.to(self.device)
                # print("diagAtA:", diagAtA.min(), diagAtA.max(), diagAtA.mean().item())
                output = model(Aty, diagAtA, mk_1, invSigmak_1, mk, invSigmak, lambdak)
                recon = output[2]

                for i in range(recon.shape[0]):
                    recon_t = recon[i, 0].detach()
                    hr_t = x_true[i, 0].detach()
                    Aty_t = Aty[i, 0].detach()
                    psnr_val = compute_psnr(recon_t, hr_t)
                    recon_np_i = recon_t.cpu().numpy()  # 仅单个样本转 CPU，开销小
                    hr_np_i = hr_t.cpu().numpy()
                    ssim_val = structural_similarity(hr_np_i, recon_np_i, data_range=1.0)
                    psnr_Aty = compute_psnr(Aty_t, hr_t)
                    epoch_psnr += psnr_val
                    epoch_ssim += ssim_val
                    count += 1
                    # 更新 epoch 内最优 PSNR
                    if psnr_val > best_psnr_in_epoch:
                        best_psnr_in_epoch = psnr_val
                        best_recon = recon[i:i + 1].clone()  # 保留 batch 中对应图
                        best_x_true = x_true[i:i + 1].clone()
                        best_y = y[i:i + 1].clone()
                        best_Aty = Aty[i:i + 1].clone()
                        best_ssim_val = ssim_val
                    # 输出batch中所有样本的结果
                    print(f"Batch {iter_num} | "
                          f"PSNR(Aty)={psnr_Aty:.2f}  | "
                          f"PSNR(Recon)={psnr_val:.2f}   |"
                          f"SSIM(Recon)={ssim_val:.2f}")
                # print(f"[DEBUG] recon range: {recon.min():.6f} ~ {recon.max():.6f}")
                # 保存batch中最后一个样本的recon
                if iter_num == 0:
                    self._save_reconstructed_image(recon, x_true, y, epoch, iter_num)

                loss = loss_func(output[2], x_true)
                # print('\n Loss Epoch {} : {:.4f}\n'.format(epoch, loss))
                train_loss += loss.item()
                loop.set_postfix({
                    "Loss": f"{loss.item():.4f}",
                    "Last PSNR": f"{psnr_val:.2f}dB",
                    "Last SSIM": f"{ssim_val:.4f}"
                })
                loss.backward()
                total_grad_norm = 0

                if iter_num % self.log_interval == 0:
                    optimizer.step()
                    optimizer.zero_grad()


            avg_loss = train_loss / len(test_loader)
            avg_psnr = epoch_psnr / count
            avg_ssim = epoch_ssim / count
            # if avg_psnr > best_model_psnr:
            #     best_model_psnr = avg_psnr
            #     torch.save(model.state_dict(), best_model_path)
            #     print(f"✅ Saved new best model (Epoch {epoch}, PSNR={best_model_psnr:.2f}dB)")

            losses.append(avg_loss)
            psnrs.append(avg_psnr)
            ssims.append(avg_ssim)
            # print('\nTraining Performance Epoch {}: Average loss: {:.4f}\n'.format(
            #     epoch, train_loss / len(test_loader)))
            print(f'Epoch {epoch}: Loss={avg_loss:.4f}, PSNR={avg_psnr:.2f}, SSIM={avg_ssim:.4f}\n')

            # === 保存当前epoch最优样本图像 ===
            if best_recon is not None:
                recon_img = (best_recon.detach().cpu().squeeze().numpy() * 255).astype(np.uint8)
                y_img = (best_y.detach().cpu().squeeze().numpy() * 255).astype(np.uint8)
                gt_img = (best_x_true.detach().cpu().squeeze().numpy() * 255).astype(np.uint8)
                psnr_Aty = compute_psnr(best_Aty, best_x_true)

                save_path = os.path.join(self.img_save_dir, 'train2', f"best_epoch_{epoch}")
                os.makedirs(save_path, exist_ok=True)

                plt.figure(figsize=(12, 4))
                plt.subplot(1, 3, 1)
                plt.imshow(gt_img, cmap='gray')
                plt.title('GT')
                plt.axis('off')

                plt.subplot(1, 3, 2)
                plt.imshow(y_img, cmap='gray')
                plt.title(f"Aty\nPSNR={psnr_Aty:.2f}dB")
                plt.axis('off')

                plt.subplot(1, 3, 3)
                plt.imshow(recon_img, cmap='gray')
                plt.title(f"Best Recon\nPSNR={best_psnr_in_epoch:.2f}dB SSIM={best_ssim_val:.4f}")
                plt.axis('off')

                plt.tight_layout()
                plt.savefig(os.path.join(save_path, 'best_recon.png'))
                plt.close()

        plot_progress()
        epochs = range(1, self.epochs + 1)
        plt.figure(figsize=(12, 4))
        plt.subplot(1, 3, 1)
        plt.plot(epochs, losses, marker='o')
        plt.title('Average Loss')
        plt.xlabel('Epoch')
        plt.grid(True)

        plt.subplot(1, 3, 2)
        plt.plot(epochs, psnrs, marker='o')
        plt.title('PSNR')
        plt.xlabel('Epoch')
        plt.grid(True)

        plt.subplot(1, 3, 3)
        plt.plot(epochs, ssims, marker='o')
        plt.title('SSIM')
        plt.xlabel('Epoch')
        plt.grid(True)

        plt.tight_layout()
        plt.show()
        # torch.save(model.state_dict(), './output/final.ckpt')
                # pred = output.argmax(dim=1, keepdim=True)
                # train_correct = pred.eq(target.view_as(pred)).sum().item()

            # # Performance metrics
            # metrics_dict = compute_metrics(model, val_loader, loss_func, self.device)
            # print('-' * 10, ' Epoch {} Iteration {} '.format(epoch, iter_num), '-' * 10, )
            # print("Accuracy \t {:.3f}".format(metrics_dict['Accuracy']))
            # print("Sensitivity \t {:.3f}".format(metrics_dict['Sensitivity']))
            # print("Specificity \t {:.3f}".format(metrics_dict['Specificity']))
            # print("Area Under ROC \t {:.3f}".format(metrics_dict['Roc_score']))
            # print("Val Loss \t {}".format(metrics_dict["Validation Loss"]))
            # print('-' * 40, )

            # # save model with the best validation accuracy
            # if metrics_dict['Accuracy'] > best_val_score:
            #     torch.save(model, "best_model.pkl")
            #     best_val_score = metrics_dict['Accuracy']

            # print the metrics for training data for the epoch
            # print('\nTraining Performance Epoch {}: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
            #     epoch, train_loss / len(train_loader.dataset), train_correct, len(train_loader.dataset),
            #            100.0 * train_correct / len(train_loader.dataset)))
            #
            # # log the accuracy and losses in tensorboard
            # writer.add_scalars("Losses", {'Train loss': train_loss / len(train_loader),
            #                               'Validation_loss': metrics_dict["Validation Loss"]},
            #                    epoch)
            # writer.add_scalars("Accuracies", {"Train Accuracy": 100.0 * train_correct / len(train_loader.dataset),
            #                                   "Valid Accuracy": 100.0 * metrics_dict["Accuracy"]}, epoch)

            # Add data to the EarlyStopper object
            # early_stopper.add_data(model, metrics_dict['Validation Loss'], metrics_dict['Accuracy'])

            # If both accuracy and loss are not improving, stop the training
            # if early_stopper.stop() == 1:
            #     break

            # if only loss is not improving, lower the learning rate
            # if early_stopper.stop() == 3:
            #     for param_group in optimizer.param_groups:
            #         self.lr *= 0.1
            #         param_group['lr'] = self.lr
            #         print('Updating the learning rate to {}'.format(self.lr))
            #         early_stopper.reset()
    # def test(self):

    # def evaluate(model, test_loader):

    def initialization(self, mk_0):
        invSigmak_1 = 100 * torch.ones(mk_0.shape).to(self.device)
        invSigmak = 100 * torch.ones(mk_0.shape).to(self.device)


        # operators
        Dhx = lambda x: x - torch.roll(x, 1, dims=3)
        Dvx = lambda x: x - torch.roll(x, 1, dims=2)
        lambdak = (torch.square(Dhx(mk_0)) + torch.square(Dvx(mk_0)) + torch.tensor(1e-10)).to(self.device)
        return invSigmak_1, invSigmak, lambdak

"""# Main"""

def main():
    print("Directory: " + os.getcwd())
    # args = get_arguments()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    args= Args(device=device, moddir="./models", outdir='./outputs',
                logdir='./logs', imgdir='./images', resdir='./results', initdir='./inits',
                num_classes=2, model='UnrollVBA', lr=0.001, optimizer='adam',
                dataset='infrared', batch_size=8, epochs=10, train_size=1.0,
                num_workers=2, loss='mse', log_interval=8)
    # args = get_arguments()
    # 数据集路径

    log_dir = "./logs"
    writer = SummaryWriter(log_dir)

    trainer = Trainer(args)
    trainer.training(writer)

if __name__ == "__main__":
    main()